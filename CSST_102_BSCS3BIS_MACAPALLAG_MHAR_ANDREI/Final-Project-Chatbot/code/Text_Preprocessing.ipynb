{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title ### **(LOGIN) Set up Hugging Face Token and Connect to Google Drive**\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Enter your Hugging Face Access Token (Write)\n",
        "token_name = 'Token Name' #@param {type:\"string\"}\n",
        "token_value = 'Token Value' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "os.environ[token_name] = token_value\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    user_info = api.whoami()\n",
        "    print(f\"Successfully authenticated as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {str(e)}\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "bLlVPZ-5dtaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Text Preprocess:\n",
        "!pip install datasets pyspellchecker\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spellchecker import SpellChecker\n",
        "from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm as tqdm_notebook\n",
        "import nltk\n",
        "import os\n",
        "from huggingface_hub import HfApi, HfFolder\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Enable tqdm for pandas\n",
        "tqdm.pandas()\n",
        "\n",
        "# Load the dataset from Hugging Face\n",
        "dataset = load_dataset(\"iZELX1/Comsci-Concepts-5k\")\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "df = df.sample(n=500, random_state=42)\n",
        "\n",
        "# Advanced Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    # Convert contractions\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'s\", \" is\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'t\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    # Remove redundant whitespaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    # Fix spacing around punctuation\n",
        "    text = re.sub(r'\\s([,.!?])', r'\\1', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "# Initialize SpellChecker\n",
        "spell = SpellChecker()\n",
        "\n",
        "# Apply preprocessing to input and output columns\n",
        "df['input'] = df['input'].apply(preprocess_text)\n",
        "df['output'] = df['output'].apply(preprocess_text)\n",
        "\n",
        "# Function to correct spelling with progress bar\n",
        "def correct_spelling(text):\n",
        "    words = word_tokenize(text)\n",
        "    corrected_words = []\n",
        "    for word in tqdm_notebook(words, desc=\"Correcting spelling\", leave=False):\n",
        "        corrected_word = spell.correction(word)\n",
        "        corrected_words.append(corrected_word if corrected_word is not None else word)\n",
        "    return ' '.join(corrected_words)\n",
        "\n",
        "# Apply spell checking with progress bar\n",
        "print(\"Applying spell checking to input column...\")\n",
        "df['input'] = df['input'].progress_apply(correct_spelling)\n",
        "print(\"Applying spell checking to output column...\")\n",
        "df['output'] = df['output'].progress_apply(correct_spelling)\n",
        "\n",
        "# Display the first few rows of the preprocessed dataset\n",
        "print(df.head())\n",
        "\n",
        "# Save the preprocessed DataFrame to a new CSV file\n",
        "output_file = \"preprocessed_dataset.csv\"\n",
        "df.to_csv(output_file, index=False)\n",
        "print(f\"Preprocessed dataset saved to {output_file}\")\n",
        "\n",
        "# Function to upload dataset to Hugging Face\n",
        "def upload_to_huggingface(file_path, repo_name, token):\n",
        "    # Initialize Hugging Face API\n",
        "    api = HfApi()\n",
        "\n",
        "    # Login to Hugging Face\n",
        "    HfFolder.save_token(token)\n",
        "\n",
        "    # Create a new dataset from the CSV file\n",
        "    dataset = Dataset.from_pandas(pd.read_csv(file_path))\n",
        "\n",
        "    # Push the dataset to the Hugging Face Hub\n",
        "    dataset.push_to_hub(repo_name, private=True)\n",
        "\n",
        "    print(f\"Dataset uploaded to Hugging Face: https://huggingface.co/datasets/{repo_name}\")\n",
        "\n",
        "# Ask user if they want to upload to Hugging Face\n",
        "upload_choice = input(\"Do you want to upload the preprocessed dataset to Hugging Face? (yes/no): \").lower()\n",
        "\n",
        "if upload_choice == 'yes':\n",
        "    hf_token = input(\"Enter your Hugging Face API token: \")\n",
        "    repo_name = input(\"Enter the desired repository name for your dataset: \")\n",
        "\n",
        "    upload_to_huggingface(output_file, repo_name, hf_token)\n",
        "else:\n",
        "    print(\"Dataset not uploaded to Hugging Face.\")\n",
        "\n",
        "print(\"Process completed.\")"
      ],
      "metadata": {
        "id": "2fUkCguCniLa",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}