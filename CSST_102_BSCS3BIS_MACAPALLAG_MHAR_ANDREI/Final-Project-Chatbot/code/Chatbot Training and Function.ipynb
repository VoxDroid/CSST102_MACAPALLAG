{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5v_qhnM0NWVt"
      },
      "source": [
        "# **`Setup Cells (REQUIRED)`**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "39mYncnsFHPy"
      },
      "outputs": [],
      "source": [
        "#@title ### **(LOGIN) Set up Hugging Face Token and Connect to Google Drive**\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#@markdown Enter your Hugging Face Access Token (Write)\n",
        "token_name = 'NaN' #@param {type:\"string\"}\n",
        "token_value = 'NaN' #@param {type:\"string\"}\n",
        "\n",
        "import os\n",
        "os.environ[token_name] = token_value\n",
        "\n",
        "from huggingface_hub import HfApi\n",
        "api = HfApi()\n",
        "\n",
        "try:\n",
        "    user_info = api.whoami()\n",
        "    print(f\"Successfully authenticated as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "GafqJYkLHa5m"
      },
      "outputs": [],
      "source": [
        "#@title ### **Import Libraries and Set Up Environment**\n",
        "\n",
        "!pip install transformers datasets torch sentencepiece accelerate evaluate\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from accelerate import Accelerator\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Accelerator instance\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")  # mixed precision setup\n",
        "\n",
        "clear_output(wait=True)\n",
        "\n",
        "print(\"\\nSetup Finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EES7lf53Nhvr"
      },
      "source": [
        "# **`Base Chatbot Training (INITIAL TRAINING)`**\n",
        "Chatbot Finetuning from scratch and repository creation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "V7fpQ5e9Ha-v"
      },
      "outputs": [],
      "source": [
        "#@title ### Step 1: Load and Preprocess Dataset\n",
        "#@markdown ## Enter your Dataset Here\n",
        "HF_dataset = \"iZELX1/Comsci-Concepts-25k\" #@param {type:\"string\"}\n",
        "#@markdown Leave blank to use default.\n",
        "select_file = \"\" #@param {type:\"string\"}\n",
        "if select_file:\n",
        "    dataset = load_dataset(HF_dataset, data_files=select_file)\n",
        "else:\n",
        "    dataset = load_dataset(HF_dataset)\n",
        "\n",
        "print(dataset['train'].column_names)\n",
        "sample_rows = dataset['train'].select(range(3))\n",
        "print(sample_rows)\n",
        "\n",
        "#@markdown Input Model to Use:\n",
        "model_name = \"gpt2-medium\" #@param [\"gpt2-medium\", \"facebook/opt-350m\", \"EleutherAI/gpt-neo-350m\", \"microsoft/DialoGPT-medium\", \"allenai/led-base-16384\", \"google/umt5-base\", \"microsoft/prophetnet-large-uncased\"]\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "#@markdown Two Column Structured Dataset Configuration:\n",
        "Input_Column = 'input' #@param {type:\"string\"}\n",
        "Output_Column = 'output' #@param {type:\"string\"}\n",
        "\n",
        "# Adjust max_length based on token limit and sample size\n",
        "def preprocess_function(examples):\n",
        "    texts = [f\"Human: {q1}\\nAI: {q2}\" for q1, q2 in zip(examples[Input_Column], examples[Output_Column])]\n",
        "    model_inputs = tokenizer(texts, truncation=True, padding='max_length', max_length=512)\n",
        "    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "xKOwoUQ9HbBX"
      },
      "outputs": [],
      "source": [
        "#@title ### Step 2: Prepare Data for Training\n",
        "#@markdown ### Set the dataset parameters:\n",
        "#@markdown Enter the number of dataset to use for training (randomized/shuffled)\n",
        "train_range = 20000 #@param {type:\"number\"}\n",
        "#@markdown Enter the index range of dataset to use for evaluation (not shuffled but index-based | Usually 10-20%)\n",
        "eval_index_start_range = 16000 #@param {type:\"number\"}\n",
        "eval_index_end_range = 20000 #@param {type:\"number\"}\n",
        "\n",
        "train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(train_range))\n",
        "eval_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(eval_index_start_range, eval_index_end_range))\n",
        "\n",
        "train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
        "eval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Q9FRjLZyHbD-"
      },
      "outputs": [],
      "source": [
        "#@title ### Step 3: Initialize the Selected Model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# Move model to Accelerator setup\n",
        "device = accelerator.device\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "a5P_JeFQIQcn"
      },
      "outputs": [],
      "source": [
        "#@title ### Step 4: Fine-tune the Model\n",
        "from transformers import Trainer, TrainingArguments\n",
        "#@markdown # Training Arguments\n",
        "# Set parameters for gradient accumulation\n",
        "#@markdown - ### Output Directory (Default ./results)\n",
        "output_directory = './results' #@param {type:\"string\"}\n",
        "#@markdown - ### Accumulate Gradients Over N Steps (Default 4)\n",
        "gradient_accumulation_steps = 4  #@param {type:\"number\"}\n",
        "#@markdown - ### Base Batch Size (Default 2)\n",
        "base_batch_size = 4  #@param {type:\"number\"}\n",
        "#@markdown - ### Original Learning Rate (Before Linear Scaling Rule | Default 1e-5)\n",
        "base_learning_rate = 1e-5  #@param {type:\"number\"}\n",
        "#@markdown - ### Number of Train Epochs (Default 5)\n",
        "train_epochs = 5 #@param {type:\"number\"}\n",
        "#@markdown - ### Warmup Steps (Default 500)\n",
        "warmup_steps = 500 #@param {type:\"number\"}\n",
        "#@markdown - ### Weight Decay (Default 0.02)\n",
        "weight_decay = 0.02 #@param {type:\"number\"}\n",
        "#@markdown - ### Logging Directory (Default ./logs)\n",
        "logging_directory = './logs' #@param {type:\"string\"}\n",
        "#@markdown - ### Logging Steps (Default 10)\n",
        "logging_steps = 10 #@param {type:\"number\"}\n",
        "#@markdown - ### Evaluation Strategy (Default epoch)\n",
        "evaluation_strategy = \"epoch\" #@param {type:\"string\"}\n",
        "#@markdown - ### Saving Strategy (Default epoch)\n",
        "save_strategy = \"epoch\" #@param {type:\"string\"}\n",
        "#@markdown - ### Load the Best Model at the end (Default True)\n",
        "load_best_model = True #@param {type:\"boolean\"}\n",
        "#@markdown - ### Enable Mixed-Precision Training (FP16 | Default True)\n",
        "mixed_precision = True #@param {type:\"boolean\"}\n",
        "#@markdown - ### Logging First Step (Default True)\n",
        "lfs = True #@param {type:\"boolean\"}\n",
        "#@markdown - ### Keep the last N checkpoints (Default 2)\n",
        "save_checkpoint = 2 #@param {type:\"number\"}\n",
        "#@markdown - ### Warmup Ratio (Default 0.1)\n",
        "warmup_r = 0.1 #@param {type:\"number\"}\n",
        "#@markdown - ### Markdown Epsilon (Default 1e-8)\n",
        "markdown_e = 1e-8 #@param {type:\"number\"}\n",
        "#@markdown - ### Gradient Clipping (Max Grad Norm | Default 1.0)\n",
        "grad_clip = 1.0 #@param {type:\"number\"}\n",
        "\n",
        "# Calculate effective batch size\n",
        "effective_batch_size = base_batch_size * gradient_accumulation_steps\n",
        "\n",
        "# Calculate new learning rate based on linear scaling rule\n",
        "new_learning_rate = base_learning_rate * (effective_batch_size / base_batch_size)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_directory,\n",
        "    num_train_epochs=train_epochs,\n",
        "    per_device_train_batch_size=base_batch_size,\n",
        "    per_device_eval_batch_size=base_batch_size,\n",
        "    warmup_steps=warmup_steps,\n",
        "    weight_decay=weight_decay,\n",
        "    logging_dir=logging_directory,\n",
        "    logging_steps=logging_steps,\n",
        "    evaluation_strategy=evaluation_strategy,\n",
        "    save_strategy=save_strategy,\n",
        "    load_best_model_at_end=load_best_model,\n",
        "    fp16=mixed_precision,\n",
        "    learning_rate=new_learning_rate,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    logging_first_step=lfs,\n",
        "    save_total_limit=save_checkpoint,\n",
        "    warmup_ratio=warmup_r,\n",
        "    adam_epsilon=markdown_e,\n",
        "    max_grad_norm=grad_clip,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "#@markdown ## Save the Model and Tokenizer\n",
        "model_name = 'fine_tuned_drei_model' #@param {type:\"string\"}\n",
        "\n",
        "trainer.train()\n",
        "model.save_pretrained(model_name)\n",
        "tokenizer.save_pretrained(model_name)\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown # Save the Base Model To Hugging Face\n",
        "save_to_hf = True #@param {type:\"boolean\"}\n",
        "\n",
        "import os\n",
        "from huggingface_hub import HfApi, Repository, login\n",
        "\n",
        "def save_initial_model(model, tokenizer, repo_name, commit_message=\"Initial model\"):\n",
        "    # Set up authentication\n",
        "    hf_token = os.environ.get(token_name)\n",
        "    if not hf_token:\n",
        "        raise ValueError(\"Hugging Face token not found in environment variables.\")\n",
        "\n",
        "    # Login to Hugging Face\n",
        "    login(token=hf_token)\n",
        "\n",
        "    # Initialize Hugging Face API\n",
        "    api = HfApi()\n",
        "\n",
        "    # Check if the repository exists, if not, create it\n",
        "    try:\n",
        "        repo_url = api.create_repo(repo_name, exist_ok=True, token=hf_token)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating repository: {e}\")\n",
        "        return\n",
        "\n",
        "    # Clone the repository\n",
        "    repo = Repository(local_dir=repo_name, clone_from=repo_url, use_auth_token=hf_token)\n",
        "\n",
        "    # Save the model and tokenizer in the root directory\n",
        "    model.save_pretrained(repo_name)\n",
        "    tokenizer.save_pretrained(repo_name)\n",
        "\n",
        "    # Commit and push changes\n",
        "    repo.push_to_hub(commit_message=commit_message)\n",
        "\n",
        "    print(f\"Initial model saved to {repo_url}\")\n",
        "\n",
        "#@markdown ## Enter Hugging Face Repository Details (Write existing repository or create one)\n",
        "HF_Username = \"iZELX1\" #@param {type:\"string\"}\n",
        "HF_Repository = \"CodePath\" #@param {type:\"string\"}\n",
        "\n",
        "if save_to_hf:\n",
        "  save_initial_model(model, tokenizer, f\"{HF_Username}/{HF_Repository}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "ioynDLiFHbGF"
      },
      "outputs": [],
      "source": [
        "#@title ### Step 5: Create Chatbot Function for Base Model (Beta)\n",
        "from huggingface_hub import HfApi\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import logging\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from typing import List, Dict, Union, Tuple\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import re\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class AdvancedChatbotManager:\n",
        "    def __init__(self, model, tokenizer, history_file: str = \"chat_history.json\"):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model.config.pad_token_id = self.model.config.eos_token_id\n",
        "        self.history_file = history_file\n",
        "        self.max_history = 15\n",
        "        self.max_repetition_threshold = 0.7\n",
        "        self.min_response_length = 15\n",
        "        self.max_response_length = 150\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        self.topic_model = self._initialize_topic_model()\n",
        "        self.user_feedback = []\n",
        "\n",
        "    def _initialize_topic_model(self):\n",
        "        dictionary = corpora.Dictionary([[\"topic\", \"model\", \"initialization\"]])\n",
        "        corpus = [dictionary.doc2bow([\"topic\", \"model\", \"initialization\"])]\n",
        "        return LdaModel(corpus=corpus, id2word=dictionary, num_topics=5, passes=1)\n",
        "\n",
        "    def load_chat_history(self) -> List[Dict]:\n",
        "        try:\n",
        "            if os.path.exists(self.history_file):\n",
        "                with open(self.history_file, 'r') as f:\n",
        "                    history = json.load(f)\n",
        "                if self._validate_history(history):\n",
        "                    return history\n",
        "        except (json.JSONDecodeError, FileNotFoundError) as e:\n",
        "            logging.warning(f\"Error loading chat history: {e}\")\n",
        "        return []\n",
        "\n",
        "    def _validate_history(self, history: List[Dict]) -> bool:\n",
        "        if not isinstance(history, list):\n",
        "            return False\n",
        "        for entry in history:\n",
        "            if not isinstance(entry, dict) or 'human' not in entry or 'ai' not in entry or 'timestamp' not in entry:\n",
        "                return False\n",
        "            if not isinstance(entry['human'], str) or not isinstance(entry['ai'], str) or not isinstance(entry['timestamp'], str):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def save_chat_history(self, history: List[Dict]) -> None:\n",
        "        try:\n",
        "            if os.path.exists(self.history_file):\n",
        "                backup_file = f\"{self.history_file}.{datetime.now().strftime('%Y%m%d%H%M%S')}.backup\"\n",
        "                os.replace(self.history_file, backup_file)\n",
        "\n",
        "            with open(self.history_file, 'w') as f:\n",
        "                json.dump(history, f, indent=2)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving chat history: {e}\")\n",
        "\n",
        "    def detect_repetition(self, response: str, history: List[Dict]) -> bool:\n",
        "        if not history:\n",
        "            return False\n",
        "\n",
        "        recent_responses = [entry['ai'] for entry in history[-5:]]\n",
        "        for past_response in recent_responses:\n",
        "            similarity = self._calculate_similarity(response, past_response)\n",
        "            if similarity > self.max_repetition_threshold:\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    def _calculate_similarity(self, text1: str, text2: str) -> float:\n",
        "        if not text1 or not text2:\n",
        "            return 0.0\n",
        "\n",
        "        words1 = set(text1.lower().split())\n",
        "        words2 = set(text2.lower().split())\n",
        "\n",
        "        intersection = len(words1.intersection(words2))\n",
        "        union = len(words1.union(words2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    def check_response_quality(self, response: str, user_input: str) -> bool:\n",
        "        if len(response.split()) < self.min_response_length:\n",
        "            return False\n",
        "        if len(response.split()) > self.max_response_length:\n",
        "            return False\n",
        "        if response.count('.') > 15:\n",
        "            return False\n",
        "        if len(set(response.split())) < len(response.split()) * 0.4:\n",
        "            return False\n",
        "        if not self._check_relevance(response, user_input):\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def _check_relevance(self, response: str, user_input: str) -> bool:\n",
        "        words1 = set(user_input.lower().split())\n",
        "        words2 = set(response.lower().split())\n",
        "\n",
        "        intersection = len(words1.intersection(words2))\n",
        "        min_overlap = 1 if len(words1) < 3 else 2\n",
        "\n",
        "        return intersection >= min_overlap\n",
        "\n",
        "    def generate_response(self, user_input: str, history: List[Dict], max_attempts: int = 5) -> str:\n",
        "        history_text = self._format_history(history)\n",
        "        input_text = f\"{history_text}\\nHuman: {user_input}\\nAI:\"\n",
        "\n",
        "        for attempt in range(max_attempts):\n",
        "            try:\n",
        "                inputs = self.tokenizer.encode_plus(\n",
        "                    input_text,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding='max_length',\n",
        "                    max_length=512,\n",
        "                    truncation=True\n",
        "                ).to(self.device)\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.model.generate(\n",
        "                        inputs['input_ids'],\n",
        "                        attention_mask=inputs['attention_mask'],\n",
        "                        max_new_tokens=150,\n",
        "                        num_return_sequences=1,\n",
        "                        no_repeat_ngram_size=3,\n",
        "                        top_k=50,\n",
        "                        top_p=0.92,\n",
        "                        temperature=self._dynamic_temperature(attempt),\n",
        "                        do_sample=True\n",
        "                    )\n",
        "\n",
        "                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "                response = response.split(\"AI:\")[-1].strip()\n",
        "\n",
        "                if (self.check_response_quality(response, user_input) and\n",
        "                    not self.detect_repetition(response, history)):\n",
        "                    return response\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error generating response (attempt {attempt+1}): {e}\")\n",
        "\n",
        "        return self._generate_fallback_response(user_input, history)\n",
        "\n",
        "    def _dynamic_temperature(self, attempt: int) -> float:\n",
        "        return 0.7 + (attempt * 0.05)\n",
        "\n",
        "    def _generate_fallback_response(self, user_input: str, history: List[Dict]) -> str:\n",
        "        fallback_responses = [\n",
        "            \"I apologize, but I'm having trouble understanding. Could you please rephrase your question?\",\n",
        "            \"I'm not sure I have enough information to answer that. Can you provide more context?\",\n",
        "            \"That's an interesting question. To better assist you, could you clarify what specific aspect you're most interested in?\",\n",
        "            \"I want to make sure I give you the most accurate information. Could you break down your question into smaller parts?\",\n",
        "            \"I'm still learning and evolving. Could you try asking your question in a different way?\"\n",
        "        ]\n",
        "        return np.random.choice(fallback_responses)\n",
        "\n",
        "    def _format_history(self, history: List[Dict]) -> str:\n",
        "        formatted = []\n",
        "        for entry in history[-self.max_history:]:\n",
        "            formatted.extend([\n",
        "                f\"Human: {entry['human']}\",\n",
        "                f\"AI: {entry['ai']}\"\n",
        "            ])\n",
        "        return \"\\n\".join(formatted)\n",
        "\n",
        "    def reset_history(self) -> List[Dict]:\n",
        "        return []\n",
        "\n",
        "    def analyze_conversation(self, history: List[Dict]) -> Dict:\n",
        "        human_words = Counter()\n",
        "        ai_words = Counter()\n",
        "        for entry in history:\n",
        "            human_words.update(entry['human'].lower().split())\n",
        "            ai_words.update(entry['ai'].lower().split())\n",
        "\n",
        "        all_text = \" \".join([entry['human'] + \" \" + entry['ai'] for entry in history])\n",
        "        bow_corpus = self.topic_model.id2word.doc2bow(all_text.lower().split())\n",
        "        topics = self.topic_model[bow_corpus]\n",
        "\n",
        "        return {\n",
        "            'total_exchanges': len(history),\n",
        "            'top_human_words': human_words.most_common(5),\n",
        "            'top_ai_words': ai_words.most_common(5),\n",
        "            'average_human_length': sum(len(entry['human'].split()) for entry in history) / len(history),\n",
        "            'average_ai_length': sum(len(entry['ai'].split()) for entry in history) / len(history),\n",
        "            'main_topics': sorted(topics, key=lambda x: x[1], reverse=True)[:3],\n",
        "        }\n",
        "\n",
        "    def handle_special_commands(self, user_input: str, history: List[Dict]) -> Tuple[bool, str]:\n",
        "        if user_input.lower() in ['reset', 'clear']:\n",
        "            return True, \"Conversation history has been reset. How can I help you?\"\n",
        "        elif user_input.lower() in ['analyze', 'stats']:\n",
        "            analysis = self.analyze_conversation(history)\n",
        "            return True, f\"Conversation Analysis:\\n{json.dumps(analysis, indent=2)}\"\n",
        "        elif user_input.lower() in ['help', 'commands']:\n",
        "            return True, \"Available commands: reset/clear, analyze/stats, help/commands, feedback\"\n",
        "        elif user_input.lower() == 'feedback':\n",
        "            return True, self._get_user_feedback()\n",
        "        return False, \"\"\n",
        "\n",
        "    def _get_user_feedback(self) -> str:\n",
        "        feedback = input(\"Please rate the chatbot's performance (1-5, 5 being best): \")\n",
        "        comment = input(\"Any additional comments? \")\n",
        "        self.user_feedback.append({\"rating\": feedback, \"comment\": comment, \"timestamp\": datetime.now().isoformat()})\n",
        "        return \"Thank you for your feedback!\"\n",
        "\n",
        "    def detect_user_intent(self, user_input: str) -> str:\n",
        "        intents = {\n",
        "            \"greeting\": [\"hello\", \"hi\", \"hey\", \"greetings\"],\n",
        "            \"farewell\": [\"bye\", \"goodbye\", \"see you\", \"farewell\"],\n",
        "            \"question\": [\"what\", \"why\", \"how\", \"when\", \"where\", \"who\"],\n",
        "            \"command\": [\"do\", \"please\", \"can you\", \"could you\"],\n",
        "            \"opinion\": [\"think\", \"believe\", \"feel\", \"opinion\"],\n",
        "        }\n",
        "\n",
        "        user_input_lower = user_input.lower()\n",
        "        for intent, keywords in intents.items():\n",
        "            if any(keyword in user_input_lower for keyword in keywords):\n",
        "                return intent\n",
        "        return \"general\"\n",
        "\n",
        "def load_base_model():\n",
        "  #@markdown Enter your Hugging Face Repository with the saved Base Model\n",
        "    repo_name = \"iZELX1/CodePath\" #@param {type:\"string\"}\n",
        "    api = HfApi()\n",
        "\n",
        "    try:\n",
        "        model_files = api.list_repo_files(repo_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing repository: {e}\")\n",
        "        return None, None\n",
        "\n",
        "    try:\n",
        "        model = AutoModelForCausalLM.from_pretrained(repo_name)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(repo_name)\n",
        "        print(f\"Base model and tokenizer loaded successfully from {repo_name}\")\n",
        "        return model, tokenizer\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading base model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def main():\n",
        "    model, tokenizer = load_base_model()\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"Failed to load the base model. Exiting.\")\n",
        "        return\n",
        "\n",
        "    chatbot = AdvancedChatbotManager(model=model, tokenizer=tokenizer)\n",
        "    chat_history = chatbot.load_chat_history()\n",
        "    print(\"Chatbot: Hello! How can I assist you today?\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \").strip()\n",
        "\n",
        "        if user_input.lower() in ['quit', 'exit', 'bye']:\n",
        "            print(\"Chatbot: Goodbye! It was a pleasure assisting you.\")\n",
        "            chatbot.save_chat_history(chat_history)\n",
        "            break\n",
        "\n",
        "        special_command, special_response = chatbot.handle_special_commands(user_input, chat_history)\n",
        "        if special_command:\n",
        "            print(f\"Chatbot: {special_response}\")\n",
        "            if user_input.lower() in ['reset', 'clear']:\n",
        "                chat_history = chatbot.reset_history()\n",
        "            continue\n",
        "\n",
        "        if len(user_input) < 2:\n",
        "            print(\"Chatbot: Could you please provide more details or ask a specific question?\")\n",
        "            continue\n",
        "\n",
        "        intent = chatbot.detect_user_intent(user_input)\n",
        "        print(f\"Detected user intent: {intent}\")\n",
        "\n",
        "        try:\n",
        "            response = chatbot.generate_response(user_input, chat_history)\n",
        "            print(\"Chatbot:\", response)\n",
        "\n",
        "            chat_history.append({\n",
        "                \"human\": user_input,\n",
        "                \"ai\": response,\n",
        "                \"timestamp\": datetime.now().isoformat(),\n",
        "                \"intent\": intent\n",
        "            })\n",
        "\n",
        "            if len(chat_history) > chatbot.max_history:\n",
        "                chat_history = chat_history[-chatbot.max_history:]\n",
        "\n",
        "            chatbot.save_chat_history(chat_history)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in main loop: {e}\")\n",
        "            print(\"Chatbot: I apologize, but I encountered an error. Could you please try again?\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "yTFPN0Ozhaxd"
      },
      "outputs": [],
      "source": [
        "#@title GPU Cleaning (Optional)\n",
        "import torch\n",
        "import gc\n",
        "\n",
        "class GPUMemoryManager:\n",
        "    @staticmethod\n",
        "    def clear_gpu_memory():\n",
        "        \"\"\"Clear GPU memory and cache\"\"\"\n",
        "        # Empty CUDA cache\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Force garbage collection\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear any remaining tensors\n",
        "        for obj in gc.get_objects():\n",
        "            try:\n",
        "                if torch.is_tensor(obj):\n",
        "                    del obj\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "        # Empty cache again after cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Print memory stats\n",
        "        if torch.cuda.is_available():\n",
        "            print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "            print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\n",
        "\n",
        "def main():\n",
        "    try:\n",
        "        # Clear GPU memory before starting\n",
        "        GPUMemoryManager.clear_gpu_memory()\n",
        "\n",
        "        print(\"Finished Cleaning.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")\n",
        "\n",
        "    finally:\n",
        "        GPUMemoryManager.clear_gpu_memory()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
