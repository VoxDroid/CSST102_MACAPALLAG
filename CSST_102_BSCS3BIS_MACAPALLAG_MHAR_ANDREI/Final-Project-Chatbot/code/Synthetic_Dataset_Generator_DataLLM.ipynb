{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6pF1iswkmqR"
      },
      "source": [
        "# Synthetic Dataset Generator\n",
        "\n",
        "[![GitHub](https://img.shields.io/badge/GitHub-@voxdroid-181717?style=for-the-badge&logo=github)](https://github.com/voxdroid)\n",
        "![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)\n",
        "![Pandas](https://img.shields.io/badge/Pandas-150458?style=for-the-badge&logo=pandas&logoColor=white)\n",
        "![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)\n",
        "![Google Colab](https://img.shields.io/badge/Colab-F9AB00?style=for-the-badge&logo=googlecolab&color=525252)\n",
        "\n",
        "This notebook generates synthetic datasets for various applications. It uses advanced AI models to create realistic and diverse data based on your specifications.\n",
        "\n",
        "## Notes\n",
        "\n",
        "- Adjust parameters as needed for your specific use case\n",
        "- Verify the quality and relevance of the generated data for your application\n",
        "\n",
        "For questions or improvements, contact: [@VoxDroid](https://github.com/voxdroid) or visit the repo: [Synthetic-Dataset-Generator-DataLLM](https://github.com/VoxDroid/Synthetic-Dataset-Generator-DataLLM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "0t-Kg8xZbUks"
      },
      "outputs": [],
      "source": [
        "# @title # **`Dataset Generation`**\n",
        "\n",
        "class tc:\n",
        "    # ANSI escape codes for text colors\n",
        "    reset = \"\\033[0m\"         # Reset to default\n",
        "    black = \"\\033[30m\"        # Black\n",
        "    red = \"\\033[31m\"          # Red\n",
        "    green = \"\\033[32m\"        # Green\n",
        "    yellow = \"\\033[33m\"       # Yellow\n",
        "    blue = \"\\033[34m\"         # Blue\n",
        "    magenta = \"\\033[35m\"      # Magenta\n",
        "    cyan = \"\\033[36m\"         # Cyan\n",
        "    white = \"\\033[37m\"        # White\n",
        "\n",
        "    # Background colors\n",
        "    bg_black = \"\\033[40m\"     # Black background\n",
        "    bg_red = \"\\033[41m\"       # Red background\n",
        "    bg_green = \"\\033[42m\"     # Green background\n",
        "    bg_yellow = \"\\033[43m\"    # Yellow background\n",
        "    bg_blue = \"\\033[44m\"      # Blue background\n",
        "    bg_magenta = \"\\033[45m\"   # Magenta background\n",
        "    bg_cyan = \"\\033[46m\"      # Cyan background\n",
        "    bg_white = \"\\033[47m\"     # White background\n",
        "\n",
        "    # Bright colors\n",
        "    bright_black = \"\\033[90m\"  # Bright Black (Gray)\n",
        "    bright_red = \"\\033[91m\"    # Bright Red\n",
        "    bright_green = \"\\033[92m\"  # Bright Green\n",
        "    bright_yellow = \"\\033[93m\" # Bright Yellow\n",
        "    bright_blue = \"\\033[94m\"   # Bright Blue\n",
        "    bright_magenta = \"\\033[95m\" # Bright Magenta\n",
        "    bright_cyan = \"\\033[96m\"   # Bright Cyan\n",
        "    bright_white = \"\\033[97m\"  # Bright White\n",
        "\n",
        "    # Text styles\n",
        "    bold = \"\\033[1m\"           # Bold\n",
        "    italic = \"\\033[3m\"         # Italic\n",
        "    underline = \"\\033[4m\"      # Underline\n",
        "\n",
        "import sys\n",
        "import importlib\n",
        "from IPython.display import clear_output\n",
        "import subprocess\n",
        "\n",
        "def check_and_install(package_name):\n",
        "    try:\n",
        "        if package_name in sys.modules:\n",
        "            print(f\"\\n{tc.blue}{package_name} is already installed and accessible from sys.modules.\\n{tc.reset}\")\n",
        "            return True\n",
        "\n",
        "        if importlib.util.find_spec(package_name) is not None:\n",
        "            print(f\"\\n{tc.blue}{package_name} is already installed and accessible.\\n{tc.reset}\")\n",
        "            return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{tc.red}An unexpected error occurred during checks: {e}{tc.reset}\")\n",
        "\n",
        "    print(f\"\\n{tc.red}{package_name} is not installed. Attempting installation...\\n{tc.reset}\")\n",
        "\n",
        "    try:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package_name])\n",
        "        clear_output()\n",
        "        print(f\"\\n{tc.green}{package_name} installed successfully.{tc.reset}\")\n",
        "\n",
        "        try:\n",
        "            module = importlib.import_module(package_name)\n",
        "            print(f\"\\n{tc.green}{package_name} has been successfully loaded after installation.\\n{tc.reset}\")\n",
        "            return True\n",
        "\n",
        "        except ModuleNotFoundError:\n",
        "            return False\n",
        "\n",
        "    except subprocess.CalledProcessError as install_error:\n",
        "        print(f\"{tc.red}Failed to install {package_name}. Installation error: {install_error}{tc.red}\")\n",
        "        return False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"{tc.red}An unexpected error occurred during installation or import: {e}{tc.reset}\")\n",
        "        return False\n",
        "\n",
        "package_name = \"DataLLM\"\n",
        "\n",
        "if check_and_install(package_name):\n",
        "    datallm = importlib.import_module(package_name)\n",
        "else:\n",
        "    print(f\"\\n{tc.green}Setup Finished.\\n{tc.reset}\")\n",
        "\n",
        "from IPython.display import clear_output\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import re\n",
        "import time\n",
        "from datallm import DataLLM\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# @markdown # **`Dataset Configuration`**\n",
        "DataLLM_apikey = \"zpka_XXXXX\"  # @param {type:\"string\"}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Enter your Mostly.AI DataLLM API Key here to access the synthetic data generation service.*</small>\n",
        "\n",
        "# Initialize DataLLM\n",
        "datallm = DataLLM(api_key=DataLLM_apikey, base_url='https://data.mostly.ai')\n",
        "\n",
        "# @markdown # **`Dataset Description`**\n",
        "data_description = \"Sample Dataset Description\"  # @param {type:\"string\"}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Provide a description of the dataset you wish to generate. This guides the data model in producing relevant synthetic data.*</small>\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# @markdown # **`User Input Column Configuration`**\n",
        "user_input_prompt = \"Sample User Input Prompt\"  # @param {type:\"string\"}\n",
        "user_input_data_type = \"string\"  # @param [\"string\", \"integer\", \"float\", \"boolean\"]\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Define the prompt that describes the type of user input you'd like to generate.*</small>\n",
        "user_input_max_tokens = 64  # @param {type:\"slider\", min:1, max:64}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set the maximum token limit for the user input, controlling the length of the generated text.*</small>\n",
        "\n",
        "# @markdown # **`Chatbot Output Column Configuration`**\n",
        "chatbot_output_prompt = \"Sample Chatbot Output Prompt\"  # @param {type:\"string\"}\n",
        "chatbot_output_data_type = \"string\"  # @param [\"string\", \"integer\", \"float\", \"boolean\"]\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Define the chatbot's response prompt to guide the model's output.*</small>\n",
        "chatbot_output_max_tokens = 64  # @param {type:\"slider\", min:1, max:64}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set the maximum token limit for the chatbot output.*</small>\n",
        "\n",
        "# @markdown # **`Intent Column Configuration`**\n",
        "intent = \"Sample Intent\" # @param {type:\"string\"}\n",
        "intent_data_type = \"string\" # @param {type:\"string\"}\n",
        "# @markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Intent represents the core purpose of the user's query.*</small>\n",
        "intent_max_tokens = 8  # @param {type:\"slider\", min:1, max:32}\n",
        "# @markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set a concise length for intent categorization.*</small>\n",
        "\n",
        "# @markdown # **`Sentiment Column Configuration`**\n",
        "sentiment = \"Sample Sentiment\" # @param {type:\"string\"}\n",
        "sentiment_data_type = \"string\" # @param {type:\"string\"}\n",
        "# @markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Sentiment reflects the user's emotional tone.*</small>\n",
        "sentiment_max_tokens = 8 # @param {type:\"slider\", min:1, max:32}\n",
        "# @markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set a concise length for sentiment categorization.*</small>\n",
        "\n",
        "# @markdown # **`Difficulty Level Column Configuration`**\n",
        "diff_level = \"Sample Difficulty Level\" # @param {type:\"string\"}\n",
        "diff_level_data_type = \"string\" # @param {type:\"string\"}\n",
        "# @markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Difficulty level corresponds to the complexity of the query.*</small>\n",
        "diff_level_max_tokens = 8  # @param {type:\"slider\", min:1, max:32}\n",
        "# @markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set a concise length for difficulty categorization.*</small>\n",
        "\n",
        "# @markdown ---\n",
        "\n",
        "# Define columns for data generation\n",
        "columns = {\n",
        "    \"input_part1\": {\n",
        "        \"prompt\": user_input_prompt,\n",
        "        \"dtype\": user_input_data_type,\n",
        "        \"max_tokens\": user_input_max_tokens\n",
        "    },\n",
        "    \"output_part1\": {\n",
        "        \"prompt\": chatbot_output_prompt,\n",
        "        \"dtype\": chatbot_output_data_type,\n",
        "        \"max_tokens\": chatbot_output_max_tokens\n",
        "    },\n",
        "    \"intent\": {\n",
        "        \"prompt\": intent,\n",
        "        \"dtype\": intent_data_type,\n",
        "        \"max_tokens\": intent_max_tokens\n",
        "    },\n",
        "    \"sentiment\": {\n",
        "        \"prompt\": sentiment,\n",
        "        \"dtype\": sentiment_data_type,\n",
        "        \"max_tokens\": sentiment_max_tokens\n",
        "    },\n",
        "    \"difficulty_level\": {\n",
        "        \"prompt\": diff_level,\n",
        "        \"dtype\": diff_level_data_type,\n",
        "        \"max_tokens\": diff_level_max_tokens\n",
        "    }\n",
        "}\n",
        "\n",
        "# @markdown # **`Dataset Structure`**\n",
        "use_custom_rows = True  # @param {type:\"boolean\"}\n",
        "data_rows_option = \"10\"  # @param [\"10\", \"100\", \"1000\", \"10000\"]\n",
        "data_rows_custom = 50  # @param {type:\"integer\"}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set the number of rows to generate. Choose predefined options (`10`, `100`, `1,000`, `10,000`) or enable custom rows.*</small>\n",
        "\n",
        "# Determine the number of rows\n",
        "data_rows = int(data_rows_option) if not use_custom_rows else data_rows_custom\n",
        "\n",
        "# @markdown # **`Additional Parts Until the Response is Complete`**\n",
        "maximum_iterations = 2  # @param {type:\"slider\", min:1, max:5}\n",
        "ap_data_type = \"string\"  # @param [\"string\", \"integer\", \"float\", \"boolean\"]\n",
        "ap_max_tokens = 64  # @param {type:\"slider\", min:1, max:64}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Set the number of iterations to complete responses if the model does not return a full sentence initially.*</small>\n",
        "\n",
        "# @markdown # **`Dataset Filename (saved as .csv file)`**\n",
        "csv_filename = \"Synthetic_Dataset\"  # @param {type:\"string\"}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Enter a name for the CSV file where the dataset will be saved.*</small>\n",
        "\n",
        "# @markdown # **`Include Timestamp in Filename`**\n",
        "include_timestamp = True  # @param {type:\"boolean\"}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Enable to add a timestamp to the filename for easier tracking of versions.*</small>\n",
        "\n",
        "# @markdown # **`Save to Google Drive`**\n",
        "save_to_gdrive = True  # @param {type:\"boolean\"}\n",
        "date = datetime.now().strftime(\"%Y%m%d\")\n",
        "gdrive_folder = f\"Dataset-Generator/Dataset-{date}\"\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Check this box to save the dataset to your Google Drive.*</small>\n",
        "\n",
        "# @markdown # **`Loop (Cycle) Generation Option`**\n",
        "number_of_cycles = 1  # @param {type:\"integer\"}\n",
        "#@markdown - <small style=\"color: gray; font-size: 0.85em;\"> **Note**: *Specify the number of cycles to generate datasets with the same configuration.*</small>\n",
        "\n",
        "# Mount Google Drive if needed\n",
        "if save_to_gdrive:\n",
        "    from google.colab import drive\n",
        "    print(f\"{tc.blue}Mounting Google Drive...\\n{tc.reset}\")\n",
        "    drive.mount('/content/drive')\n",
        "    print(f\"{tc.green}\\nSuccessfully mounted to Google Drive.{tc.reset}\")\n",
        "\n",
        "# Function to check if the text ends with a complete sentence\n",
        "def is_complete_sentence(text):\n",
        "    return bool(re.search(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\!|\\?)\\s*$', text))\n",
        "\n",
        "# Function to generate additional parts until the response is complete\n",
        "def generate_complete_text(initial_text, prompt, data_description, max_iterations=maximum_iterations):\n",
        "    complete_text = initial_text\n",
        "    iterations = 0\n",
        "    while not is_complete_sentence(complete_text) and iterations < max_iterations:\n",
        "        try:\n",
        "            additional_part = datallm.enrich(\n",
        "                data=pd.DataFrame({'text': [complete_text]}),\n",
        "                prompt=prompt,\n",
        "                data_description=data_description,\n",
        "                dtype=ap_data_type,\n",
        "                max_tokens=ap_max_tokens,\n",
        "                progress_bar=False\n",
        "            )\n",
        "            additional_text = additional_part.iloc[0]\n",
        "            complete_text += \" \" + additional_text\n",
        "            iterations += 1\n",
        "        except Exception as e:\n",
        "            print(f\"{tc.red}Error while enriching text: {e}{tc.reset}\")\n",
        "            break\n",
        "    return complete_text\n",
        "\n",
        "print(f\"\\n{tc.blue}{tc.bold}Preparing to generate datasets...\\n{tc.reset}\")\n",
        "\n",
        "# Loop to generate multiple datasets\n",
        "for cycle in range(1, number_of_cycles + 1):\n",
        "    print(f\"\\n{tc.cyan}Generating dataset cycle {cycle} of {number_of_cycles}...\\n{tc.reset}\")\n",
        "\n",
        "    try:\n",
        "        time.sleep(1)\n",
        "        synthetic_data = datallm.mock(\n",
        "            n=data_rows,\n",
        "            data_description=data_description,\n",
        "            columns=columns,\n",
        "            progress_bar=True\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"{tc.red}Error while generating synthetic data: {e}{tc.reset}\")\n",
        "        continue\n",
        "\n",
        "    # Generate complete input and output\n",
        "    print(f\"\\n{tc.blue}Proceeding to process datasets...\\n{tc.reset}\")\n",
        "\n",
        "    with tqdm(total=len(synthetic_data), desc=f\"{tc.cyan}Processing dataset\") as pbar:\n",
        "        for i in range(len(synthetic_data)):\n",
        "            synthetic_data.at[i, 'input'] = generate_complete_text(\n",
        "                synthetic_data.at[i, 'input_part1'],\n",
        "                \"Continue the student query.\",\n",
        "                data_description\n",
        "            )\n",
        "            synthetic_data.at[i, 'output'] = generate_complete_text(\n",
        "                synthetic_data.at[i, 'output_part1'],\n",
        "                \"Continue the chatbot response.\",\n",
        "                data_description\n",
        "            )\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Drop the part columns\n",
        "    synthetic_data = synthetic_data.drop(columns=['input_part1', 'output_part1'])\n",
        "\n",
        "    # Add an ID column to the dataset\n",
        "    synthetic_data.insert(0, 'id', range(1, len(synthetic_data) + 1))\n",
        "\n",
        "    column_order = ['id', 'input', 'output', 'intent', 'sentiment', 'difficulty_level']\n",
        "    synthetic_data = synthetic_data[column_order]\n",
        "\n",
        "    # Save to CSV with optional timestamp\n",
        "    timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\") if include_timestamp else \"\"\n",
        "    csv_path = f'Dataset-Generator/Dataset-{date}/{csv_filename}_{cycle}{f\"_{timestamp}\" if include_timestamp else \"\"}.csv'\n",
        "\n",
        "    try:\n",
        "        directory = os.path.dirname(csv_path)\n",
        "        if not os.path.exists(directory):\n",
        "            os.makedirs(directory)\n",
        "        synthetic_data.to_csv(csv_path, index=False)\n",
        "        print(f\"\\n\\n{tc.green}Dataset cycle {cycle} saved successfully at: {csv_path}{tc.reset}\")\n",
        "\n",
        "        # Optionally save to Google Drive\n",
        "        if save_to_gdrive:\n",
        "            gdrive_path = f'/content/drive/My Drive/{gdrive_folder}/{csv_filename}_{cycle}{f\"_{timestamp}\" if include_timestamp else \"\"}.csv'\n",
        "            os.makedirs(os.path.dirname(gdrive_path), exist_ok=True)\n",
        "            synthetic_data.to_csv(gdrive_path, index=False)\n",
        "            print(f\"\\n{tc.green}Dataset cycle {cycle} also saved to Google Drive at: {gdrive_path}{tc.reset}\")\n",
        "    except Exception as e:\n",
        "        print(f\"{tc.red}Error while saving dataset: {e}{tc.reset}\")\n",
        "\n",
        "print(f\"\\n{tc.bright_green}{tc.bold}Dataset generation finished.{tc.green}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
