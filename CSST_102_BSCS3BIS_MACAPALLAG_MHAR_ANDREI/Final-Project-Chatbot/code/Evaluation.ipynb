{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "3z5NqFlJXFh9",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "#@title Evaluation\n",
        "!pip install datasets\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Check for GPU availability\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "model_name = \"iZELX1/CodePath\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"iZELX1/Comsci-Concepts-25k\")\n",
        "eval_dataset = dataset['train'].select(range(16000, 20000))  # Using the same range as in the training\n",
        "\n",
        "def generate_response(input_text):\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=50, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "def evaluate_model(dataset):\n",
        "    true_labels = []\n",
        "    predicted_labels = []\n",
        "    bleu_scores = []\n",
        "\n",
        "    for example in tqdm(dataset):\n",
        "        input_text = f\"Human: {example['input']}\\nAI:\"\n",
        "        response = generate_response(input_text)\n",
        "\n",
        "        # Simple classification based on response content\n",
        "        true_label = 1 if example['output'] in example['input'] else 0\n",
        "        pred_label = 1 if example['output'] in response else 0\n",
        "\n",
        "        true_labels.append(true_label)\n",
        "        predicted_labels.append(pred_label)\n",
        "\n",
        "        # Calculate BLEU score\n",
        "        reference = nltk.word_tokenize(example['output'])\n",
        "        candidate = nltk.word_tokenize(response)\n",
        "        bleu_score = sentence_bleu([reference], candidate, smoothing_function=SmoothingFunction().method1)\n",
        "        bleu_scores.append(bleu_score)\n",
        "\n",
        "    return true_labels, predicted_labels, bleu_scores\n",
        "\n",
        "print(\"Evaluating model...\")\n",
        "true_labels, predicted_labels, bleu_scores = evaluate_model(eval_dataset)\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision = precision_score(true_labels, predicted_labels, average='binary')\n",
        "recall = recall_score(true_labels, predicted_labels, average='binary')\n",
        "f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
        "avg_bleu = np.mean(bleu_scores)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(true_labels, predicted_labels)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.ylabel('True Label')\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "fpr, tpr, _ = roc_curve(true_labels, predicted_labels)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n",
        "\n",
        "# Response Length Distribution\n",
        "response_lengths = [len(generate_response(f\"Human: {ex['input']}\\nAI:\").split()) for ex in tqdm(eval_dataset)]\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(response_lengths, bins=30, edgecolor='black')\n",
        "plt.title('Distribution of Response Lengths')\n",
        "plt.xlabel('Number of Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# BLEU Score Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(bleu_scores, bins=30, edgecolor='black')\n",
        "plt.title('Distribution of BLEU Scores')\n",
        "plt.xlabel('BLEU Score')\n",
        "plt.ylabel('Frequency')\n",
        "plt.show()\n",
        "\n",
        "# Additional Metrics\n",
        "# Remove the problematic line and replace with proper perplexity calculation\n",
        "def calculate_perplexity(dataset, model, tokenizer, device, batch_size=4):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_length = 0\n",
        "\n",
        "    print(f\"Type of dataset: {type(dataset)}\")\n",
        "    print(f\"Length of dataset: {len(dataset)}\")\n",
        "\n",
        "    # Convert dataset to a list if it's not already\n",
        "    dataset_list = list(dataset)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in tqdm(range(0, len(dataset_list), batch_size), desc=\"Calculating perplexity\"):\n",
        "            batch = dataset_list[i:i + batch_size]\n",
        "\n",
        "            # Debug: Print information about the batch\n",
        "            print(f\"Batch size: {len(batch)}\")\n",
        "            print(f\"Type of first item in batch: {type(batch[0])}\")\n",
        "            print(f\"Keys in first item: {batch[0].keys() if isinstance(batch[0], dict) else 'Not a dictionary'}\")\n",
        "\n",
        "            # Check if the items are dictionaries with 'input' and 'output' keys\n",
        "            if isinstance(batch[0], dict) and 'input' in batch[0] and 'output' in batch[0]:\n",
        "                inputs = tokenizer(\n",
        "                    [f\"Human: {ex['input']}\\nAI: {ex['output']}\" for ex in batch],\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=512\n",
        "                ).to(device)\n",
        "            else:\n",
        "                # If not, assume the items are strings and use them directly\n",
        "                inputs = tokenizer(\n",
        "                    batch,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=512\n",
        "                ).to(device)\n",
        "\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item() * inputs[\"input_ids\"].size(0)\n",
        "            total_length += inputs[\"input_ids\"].size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_length\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return perplexity.item()\n",
        "\n",
        "# Calculate perplexity\n",
        "print(\"Calculating perplexity...\")\n",
        "perplexity = calculate_perplexity(eval_dataset, model, tokenizer, device)\n",
        "print(f\"Perplexity: {perplexity:.4f}\")\n",
        "\n",
        "# Token-level accuracy\n",
        "def calculate_token_accuracy(dataset):\n",
        "    total_correct = 0\n",
        "    total_tokens = 0\n",
        "\n",
        "    for example in tqdm(dataset):\n",
        "        input_text = f\"Human: {example['input']}\\nAI:\"\n",
        "        target_text = example['output']\n",
        "\n",
        "        # Tokenize input and target separately\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512)['input_ids'].to(device)\n",
        "        target_ids = tokenizer(target_text, return_tensors=\"pt\", truncation=True, max_length=512)['input_ids'].to(device)\n",
        "\n",
        "        # Ensure input_ids and target_ids have the same length\n",
        "        max_length = max(input_ids.size(1), target_ids.size(1))\n",
        "        input_ids = torch.nn.functional.pad(input_ids, (0, max_length - input_ids.size(1)), value=tokenizer.pad_token_id)\n",
        "        target_ids = torch.nn.functional.pad(target_ids, (0, max_length - target_ids.size(1)), value=tokenizer.pad_token_id)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids=input_ids, labels=target_ids)\n",
        "            logits = outputs.logits\n",
        "\n",
        "        predictions = torch.argmax(logits, dim=-1)\n",
        "        correct = (predictions == target_ids).sum().item()\n",
        "        total_correct += correct\n",
        "        total_tokens += target_ids.numel()\n",
        "\n",
        "    return total_correct / total_tokens\n",
        "\n",
        "# Calculate token-level accuracy\n",
        "print(\"Calculating token-level accuracy...\")\n",
        "token_accuracy = calculate_token_accuracy(eval_dataset)\n",
        "print(f\"Token-level Accuracy: {token_accuracy:.4f}\")\n",
        "\n",
        "# Print all metrics\n",
        "print(\"\\nFinal Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
        "print(f\"Perplexity: {perplexity:.4f}\")\n",
        "print(f\"Token-level Accuracy: {token_accuracy:.4f}\")\n",
        "print(f\"AUC-ROC: {roc_auc:.4f}\")\n",
        "\n",
        "# BLEU Score percentiles\n",
        "bleu_percentiles = np.percentile(bleu_scores, [25, 50, 75])\n",
        "print(\"\\nBLEU Score Percentiles:\")\n",
        "print(f\"25th Percentile: {bleu_percentiles[0]:.4f}\")\n",
        "print(f\"50th Percentile (Median): {bleu_percentiles[1]:.4f}\")\n",
        "print(f\"75th Percentile: {bleu_percentiles[2]:.4f}\")\n",
        "\n",
        "# GPU Memory Management\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"\\nGPU Memory Usage:\")\n",
        "    print(f\"Allocated: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n",
        "    print(f\"Cached: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")"
      ]
    }
  ]
}